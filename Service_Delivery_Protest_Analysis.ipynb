{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service Delivery Protest Prediction Model\n",
    "## Predicting Service Delivery Protests using Census 2022 Data\n",
    "\n",
    "**Competition Submission - Data Science Challenge**\n",
    "\n",
    "### Executive Summary\n",
    "This notebook presents a comprehensive machine learning solution to predict service delivery protests in South Africa using Census 2022 demographic data and historical protest events. The model achieves an R¬≤ score of 0.652 with Random Forest as the best performing algorithm.\n",
    "\n",
    "### Table of Contents\n",
    "1. [Data Understanding & Exploration](#data-understanding)\n",
    "2. [Data Cleaning & Preprocessing](#data-cleaning)\n",
    "3. [Exploratory Data Analysis](#eda)\n",
    "4. [Feature Engineering](#feature-engineering)\n",
    "5. [Model Development](#model-development)\n",
    "6. [Model Evaluation](#model-evaluation)\n",
    "7. [Results & Interpretation](#results)\n",
    "8. [Model Deployment](#deployment)\n",
    "9. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('‚úÖ Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Understanding & Exploration {#data-understanding}\n",
    "\n",
    "### 1.1 Dataset Overview\n",
    "\n",
    "**Primary Datasets:**\n",
    "- **Census 2022 Data**: Comprehensive demographic and infrastructure data across South African municipalities\n",
    "- **Protest Events Data**: Historical demonstration and political violence events\n",
    "\n",
    "**Research Question:**\n",
    "Can we predict the likelihood of service delivery protests based on demographic and infrastructure indicators from Census 2022 data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore Census 2022 data\n",
    "import openpyxl\n",
    "\n",
    "# Load Census data\n",
    "census_file = 'Datasets/Census 2022_Themes_24-10-2023.xlsx'\n",
    "\n",
    "# Get sheet names\n",
    "wb = openpyxl.load_workbook(census_file)\n",
    "sheet_names = wb.sheetnames\n",
    "\n",
    "print('üìä Census 2022 Data Structure:')\n",
    "print(f'Total sheets: {len(sheet_names)}')\n",
    "for i, sheet in enumerate(sheet_names, 1):\n",
    "    print(f'{i:2d}. {sheet}')\n",
    "\n",
    "# Load key demographic sheets\n",
    "population_df = pd.read_excel(census_file, sheet_name='Total population')\n",
    "water_df = pd.read_excel(census_file, sheet_name='Water')\n",
    "sanitation_df = pd.read_excel(census_file, sheet_name='Sanitation')\n",
    "\n",
    "print(f'\n",
    "üìà Data Dimensions:')\n",
    "print(f'Population data: {population_df.shape}')\n",
    "print(f'Water access data: {water_df.shape}')\n",
    "print(f'Sanitation data: {sanitation_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load protest events data\n",
    "events_file = 'Datasets/Events&Fatalaties.xlsx'\n",
    "demo_file = 'Datasets/south-africa_demonstration_events_by_month-year_as-of-13aug2025.xlsx'\n",
    "\n",
    "# Load demonstration events\n",
    "demo_df = pd.read_excel(demo_file, sheet_name='Data')\n",
    "events_df = pd.read_excel(events_file, sheet_name='Data')\n",
    "\n",
    "print('üî• Protest Events Data:')\n",
    "print(f'Demonstration events: {demo_df.shape}')\n",
    "print(f'Events & fatalities: {events_df.shape}')\n",
    "\n",
    "print('\n",
    "üìÖ Time Range:')\n",
    "print(f'Demo events: {demo_df[\"Year\"].min()} - {demo_df[\"Year\"].max()}')\n",
    "print(f'Events data: {events_df[\"Year\"].min()} - {events_df[\"Year\"].max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning & Preprocessing {#data-cleaning}\n",
    "\n",
    "### 2.1 Data Quality Assessment\n",
    "\n",
    "Before building our predictive model, we need to assess and clean the data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "def assess_data_quality(df, name):\n",
    "    print(f'\n",
    "üîç Data Quality Assessment: {name}')\n",
    "    print(f'Shape: {df.shape}')\n",
    "    print(f'Missing values: {df.isnull().sum().sum()}')\n",
    "    print(f'Duplicate rows: {df.duplicated().sum()}')\n",
    "    print(f'Data types: {df.dtypes.value_counts().to_dict()}')\n",
    "    \n",
    "    # Check for non-numeric values in numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f'Numeric columns: {len(numeric_cols)}')\n",
    "        \n",
    "# Assess each dataset\n",
    "assess_data_quality(population_df, 'Population Data')\n",
    "assess_data_quality(water_df, 'Water Access Data')\n",
    "assess_data_quality(demo_df, 'Demonstration Events')\n",
    "assess_data_quality(events_df, 'Events & Fatalities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning functions\n",
    "def clean_census_data(df):\n",
    "    \"\"\"Clean census data by handling missing values and data types\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    numeric_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "    for col in numeric_cols:\n",
    "        if col not in ['Province name', 'Municipality name']:\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "    \n",
    "    # Fill missing values with median for numeric columns\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    df_clean[numeric_cols] = df_clean[numeric_cols].fillna(df_clean[numeric_cols].median())\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean all census datasets\n",
    "population_clean = clean_census_data(population_df)\n",
    "water_clean = clean_census_data(water_df)\n",
    "sanitation_clean = clean_census_data(sanitation_df)\n",
    "\n",
    "print('‚úÖ Census data cleaned successfully')\n",
    "print(f'Population data: {population_clean.shape}')\n",
    "print(f'Water data: {water_clean.shape}')\n",
    "print(f'Sanitation data: {sanitation_clean.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis {#eda}\n",
    "\n",
    "### 3.1 Demographic Patterns\n",
    "\n",
    "Let's explore the demographic and infrastructure patterns across South African provinces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provincial population analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Population by province\n",
    "pop_by_province = population_clean.groupby('Province name')['Total'].sum().sort_values(ascending=False)\n",
    "axes[0,0].bar(range(len(pop_by_province)), pop_by_province.values)\n",
    "axes[0,0].set_title('Total Population by Province')\n",
    "axes[0,0].set_ylabel('Population')\n",
    "axes[0,0].set_xticks(range(len(pop_by_province)))\n",
    "axes[0,0].set_xticklabels(pop_by_province.index, rotation=45, ha='right')\n",
    "\n",
    "# Water access patterns\n",
    "if 'Piped_water_inside_dwelling' in water_clean.columns:\n",
    "    water_access = water_clean.groupby('Province name')['Piped_water_inside_dwelling'].mean()\n",
    "    axes[0,1].bar(range(len(water_access)), water_access.values)\n",
    "    axes[0,1].set_title('Average Water Access by Province')\n",
    "    axes[0,1].set_ylabel('Piped Water Access')\n",
    "    axes[0,1].set_xticks(range(len(water_access)))\n",
    "    axes[0,1].set_xticklabels(water_access.index, rotation=45, ha='right')\n",
    "\n",
    "# Protest events over time\n",
    "events_by_year = demo_df.groupby('Year')['Events'].sum()\n",
    "axes[1,0].plot(events_by_year.index, events_by_year.values, marker='o')\n",
    "axes[1,0].set_title('Demonstration Events Over Time')\n",
    "axes[1,0].set_xlabel('Year')\n",
    "axes[1,0].set_ylabel('Number of Events')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Service delivery correlation heatmap\n",
    "service_cols = [col for col in water_clean.columns if any(keyword in col.lower() for keyword in ['piped', 'flush', 'electricity'])]\n",
    "if len(service_cols) > 1:\n",
    "    corr_matrix = water_clean[service_cols[:5]].corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1,1])\n",
    "    axes[1,1].set_title('Service Delivery Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eda_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Service Delivery Infrastructure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service delivery gap analysis\n",
    "def calculate_service_gaps(df, service_type):\n",
    "    \"\"\"Calculate service delivery gaps\"\"\"\n",
    "    if service_type == 'water':\n",
    "        good_service_cols = [col for col in df.columns if 'piped' in col.lower() and 'inside' in col.lower()]\n",
    "    elif service_type == 'sanitation':\n",
    "        good_service_cols = [col for col in df.columns if 'flush' in col.lower()]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    if good_service_cols:\n",
    "        df['Good_Service_Pct'] = df[good_service_cols].sum(axis=1) / df['Total'] * 100\n",
    "        df['Service_Gap'] = 100 - df['Good_Service_Pct']\n",
    "        return df[['Province name', 'Municipality name', 'Good_Service_Pct', 'Service_Gap']]\n",
    "    return None\n",
    "\n",
    "# Calculate service gaps\n",
    "water_gaps = calculate_service_gaps(water_clean, 'water')\n",
    "sanitation_gaps = calculate_service_gaps(sanitation_clean, 'sanitation')\n",
    "\n",
    "if water_gaps is not None:\n",
    "    print('üíß Water Service Delivery Gaps:')\n",
    "    print(water_gaps.groupby('Province name')['Service_Gap'].mean().sort_values(ascending=False))\n",
    "\n",
    "if sanitation_gaps is not None:\n",
    "    print('\n",
    "üöΩ Sanitation Service Delivery Gaps:')\n",
    "    print(sanitation_gaps.groupby('Province name')['Service_Gap'].mean().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering {#feature-engineering}\n",
    "\n",
    "### 4.1 Service Delivery Features\n",
    "\n",
    "We'll create comprehensive features that capture service delivery quality across multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main predictor class\n",
    "exec(open('service_delivery_protest_predictor.py').read())\n",
    "\n",
    "# Initialize the predictor\n",
    "predictor = ServiceDeliveryProtestPredictor()\n",
    "\n",
    "# Load and process all data\n",
    "print('üîÑ Loading and processing data...')\n",
    "census_data = predictor.load_census_data()\n",
    "protest_data = predictor.load_protest_data()\n",
    "\n",
    "print(f'‚úÖ Census data loaded: {len(census_data)} municipalities')\n",
    "print(f'‚úÖ Protest data loaded: {len(protest_data)} records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create service delivery features\n",
    "print('üõ†Ô∏è Engineering service delivery features...')\n",
    "\n",
    "# Process each service type\n",
    "water_features = predictor.create_service_delivery_features(census_data['water'], 'water')\n",
    "sanitation_features = predictor.create_service_delivery_features(census_data['sanitation'], 'sanitation')\n",
    "electricity_features = predictor.create_service_delivery_features(census_data['electricity'], 'electricity')\n",
    "refuse_features = predictor.create_service_delivery_features(census_data['refuse'], 'refuse')\n",
    "\n",
    "print(f'Water features: {water_features.shape}')\n",
    "print(f'Sanitation features: {sanitation_features.shape}')\n",
    "print(f'Electricity features: {electricity_features.shape}')\n",
    "print(f'Refuse features: {refuse_features.shape}')\n",
    "\n",
    "# Display feature examples\n",
    "print('\n",
    "üìä Sample Water Access Features:')\n",
    "print(water_features[['Municipality', 'Water_Access_Pct', 'Water_Service_Gap']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features and aggregate to provincial level\n",
    "print('üîó Combining features and aggregating to provincial level...')\n",
    "\n",
    "# Combine all service delivery features\n",
    "combined_data = predictor.combine_data_for_modeling(\n",
    "    census_data, water_features, sanitation_features, \n",
    "    electricity_features, refuse_features, protest_data\n",
    ")\n",
    "\n",
    "print(f'‚úÖ Combined dataset: {combined_data.shape}')\n",
    "print(f'Features: {list(combined_data.columns)}')\n",
    "\n",
    "# Display sample of engineered features\n",
    "print('\n",
    "üìà Sample of Engineered Features:')\n",
    "feature_cols = ['Service_Gap', 'Water_Access_Pct', 'Sanitation_Access_Pct', 'Electricity_Access_Pct']\n",
    "available_features = [col for col in feature_cols if col in combined_data.columns]\n",
    "if available_features:\n",
    "    print(combined_data[available_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature Importance and Selection\n",
    "\n",
    "Let's analyze which features are most predictive of protest risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation analysis\n",
    "numeric_features = combined_data.select_dtypes(include=[np.number])\n",
    "\n",
    "if len(numeric_features.columns) > 1:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation_matrix = numeric_features.corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, square=True, fmt='.2f')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_correlations.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify highly correlated features\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "                high_corr_pairs.append((\n",
    "                    correlation_matrix.columns[i], \n",
    "                    correlation_matrix.columns[j], \n",
    "                    correlation_matrix.iloc[i, j]\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print('‚ö†Ô∏è Highly Correlated Feature Pairs (|r| > 0.8):')\n",
    "        for feat1, feat2, corr in high_corr_pairs:\n",
    "            print(f'   {feat1} ‚Üî {feat2}: {corr:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Development {#model-development}\n",
    "\n",
    "### 5.1 Model Selection and Training\n",
    "\n",
    "We'll compare multiple algorithms to find the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "print('üéØ Preparing features for modeling...')\n",
    "\n",
    "X, y, feature_names, scaler = predictor.prepare_features_for_modeling(combined_data)\n",
    "\n",
    "print(f'Feature matrix shape: {X.shape}')\n",
    "print(f'Target vector shape: {y.shape}')\n",
    "print(f'Feature names: {feature_names}')\n",
    "print(f'Target statistics: Mean={y.mean():.2f}, Std={y.std():.2f}, Range=[{y.min():.2f}, {y.max():.2f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "print('ü§ñ Training multiple models...')\n",
    "\n",
    "models, results = predictor.train_models(X, y)\n",
    "\n",
    "# Display model performance\n",
    "print('\n",
    "üìä Model Performance Comparison:')\n",
    "for model_name, metrics in results.items():\n",
    "    print(f'{model_name:20s}: R¬≤ = {metrics[\"r2\"]:.3f}, RMSE = {metrics[\"rmse\"]:.3f}, MAE = {metrics[\"mae\"]:.3f}')\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['r2'])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f'\n",
    "üèÜ Best Model: {best_model_name} (R¬≤ = {results[best_model_name][\"r2\"]:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance\n",
    "importance_df = predictor.analyze_feature_importance(best_model, feature_names)\n",
    "\n",
    "print('üîç Top 10 Most Important Features:')\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(10)\n",
    "plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 10 Feature Importance - Service Delivery Protest Prediction')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation {#model-evaluation}\n",
    "\n",
    "### 6.1 Cross-Validation and Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model evaluation\n",
    "exec(open('model_evaluation.py').read())\n",
    "\n",
    "# Initialize evaluator with our data\n",
    "evaluator = ModelEvaluator()\n",
    "evaluator.model = best_model\n",
    "evaluator.data = combined_data\n",
    "\n",
    "# Generate evaluation report\n",
    "evaluation_results = evaluator.generate_evaluation_report()\n",
    "\n",
    "print('‚úÖ Comprehensive model evaluation completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model Validation and Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation analysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='r2')\n",
    "cv_rmse = np.sqrt(-cross_val_score(best_model, X, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "\n",
    "print('üîÑ Cross-Validation Results:')\n",
    "print(f'R¬≤ scores: {cv_scores}')\n",
    "print(f'Mean R¬≤: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}')\n",
    "print(f'RMSE scores: {cv_rmse}')\n",
    "print(f'Mean RMSE: {cv_rmse.mean():.3f} ¬± {cv_rmse.std():.3f}')\n",
    "\n",
    "# Model stability assessment\n",
    "if cv_scores.std() < 0.1:\n",
    "    print('‚úÖ Model shows good stability across folds')\n",
    "else:\n",
    "    print('‚ö†Ô∏è Model shows some instability - consider more data or regularization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results & Interpretation {#results}\n",
    "\n",
    "### 7.1 Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for analysis\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "# Create comprehensive results visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Actual vs Predicted\n",
    "axes[0,0].scatter(y, y_pred, alpha=0.6)\n",
    "min_val, max_val = min(y.min(), y_pred.min()), max(y.max(), y_pred.max())\n",
    "axes[0,0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "axes[0,0].set_xlabel('Actual Protest Risk')\n",
    "axes[0,0].set_ylabel('Predicted Protest Risk')\n",
    "axes[0,0].set_title(f'Actual vs Predicted (R¬≤ = {r2_score(y, y_pred):.3f})')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residuals\n",
    "residuals = y - y_pred\n",
    "axes[0,1].scatter(y_pred, residuals, alpha=0.6)\n",
    "axes[0,1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0,1].set_xlabel('Predicted Values')\n",
    "axes[0,1].set_ylabel('Residuals')\n",
    "axes[0,1].set_title('Residual Plot')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Model comparison\n",
    "model_names = list(results.keys())\n",
    "r2_scores = [results[name]['r2'] for name in model_names]\n",
    "axes[1,0].bar(model_names, r2_scores)\n",
    "axes[1,0].set_ylabel('R¬≤ Score')\n",
    "axes[1,0].set_title('Model Performance Comparison')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Feature importance (top 8)\n",
    "top_8_features = importance_df.head(8)\n",
    "axes[1,1].barh(range(len(top_8_features)), top_8_features['Importance'])\n",
    "axes[1,1].set_yticks(range(len(top_8_features)))\n",
    "axes[1,1].set_yticklabels(top_8_features['Feature'])\n",
    "axes[1,1].set_xlabel('Importance')\n",
    "axes[1,1].set_title('Top 8 Feature Importance')\n",
    "axes[1,1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_results_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Provincial Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provincial risk predictions\n",
    "provinces = ['Western Cape', 'Eastern Cape', 'Northern Cape', 'Free State', \n",
    "            'KwaZulu-Natal', 'North West', 'Gauteng', 'Mpumalanga', 'Limpopo']\n",
    "\n",
    "provincial_predictions = {}\n",
    "\n",
    "print('üó∫Ô∏è Provincial Protest Risk Predictions:')\n",
    "print('=' * 50)\n",
    "\n",
    "for province in provinces:\n",
    "    # Use the predictor's prediction method\n",
    "    risk_score = predictor.predict_protest_risk(province, 2024)\n",
    "    provincial_predictions[province] = risk_score\n",
    "    \n",
    "    # Risk level classification\n",
    "    if risk_score < 3:\n",
    "        risk_level = 'Low üü¢'\n",
    "    elif risk_score < 6:\n",
    "        risk_level = 'Medium üü°'\n",
    "    else:\n",
    "        risk_level = 'High üî¥'\n",
    "    \n",
    "    print(f'{province:15s}: {risk_score:5.2f} ({risk_level})')\n",
    "\n",
    "# Visualize provincial risks\n",
    "plt.figure(figsize=(12, 8))\n",
    "provinces_sorted = sorted(provincial_predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "provinces_names = [p[0] for p in provinces_sorted]\n",
    "risk_scores = [p[1] for p in provinces_sorted]\n",
    "\n",
    "colors = ['red' if score > 6 else 'orange' if score > 3 else 'green' for score in risk_scores]\n",
    "\n",
    "plt.bar(range(len(provinces_names)), risk_scores, color=colors, alpha=0.7)\n",
    "plt.xticks(range(len(provinces_names)), provinces_names, rotation=45, ha='right')\n",
    "plt.ylabel('Protest Risk Score')\n",
    "plt.title('Service Delivery Protest Risk by Province (2024)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add risk level legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='green', alpha=0.7, label='Low Risk (< 3)'),\n",
    "                  Patch(facecolor='orange', alpha=0.7, label='Medium Risk (3-6)'),\n",
    "                  Patch(facecolor='red', alpha=0.7, label='High Risk (> 6)')]\n",
    "plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('provincial_risk_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Deployment {#deployment}\n",
    "\n",
    "### 8.1 Interactive Dashboard\n",
    "\n",
    "We've created a Streamlit dashboard for interactive exploration and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and data for deployment\n",
    "import pickle\n",
    "\n",
    "# Save the best model\n",
    "with open('best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save the combined data\n",
    "with open('combined_data.pkl', 'wb') as f:\n",
    "    pickle.dump(combined_data, f)\n",
    "\n",
    "# Save feature names and scaler\n",
    "with open('model_artifacts.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'feature_names': feature_names,\n",
    "        'scaler': scaler,\n",
    "        'model_performance': results[best_model_name]\n",
    "    }, f)\n",
    "\n",
    "print('üíæ Model and artifacts saved for deployment:')\n",
    "print('   ‚Ä¢ best_model.pkl - Trained model')\n",
    "print('   ‚Ä¢ combined_data.pkl - Processed dataset')\n",
    "print('   ‚Ä¢ model_artifacts.pkl - Feature names and scaler')\n",
    "\n",
    "print('\n",
    "üöÄ Deployment Information:')\n",
    "print('   ‚Ä¢ Streamlit Dashboard: streamlit_app.py')\n",
    "print('   ‚Ä¢ Run with: streamlit run streamlit_app.py')\n",
    "print('   ‚Ä¢ Access at: http://localhost:8501')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 API Integration Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the model for predictions\n",
    "def make_prediction_example(province, year):\n",
    "    \"\"\"Example function showing how to make predictions\"\"\"\n",
    "    try:\n",
    "        # Load the saved model\n",
    "        with open('best_model.pkl', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        # Use the predictor for new predictions\n",
    "        risk_score = predictor.predict_protest_risk(province, year)\n",
    "        \n",
    "        return {\n",
    "            'province': province,\n",
    "            'year': year,\n",
    "            'risk_score': risk_score,\n",
    "            'risk_level': 'High' if risk_score > 6 else 'Medium' if risk_score > 3 else 'Low'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Test the prediction function\n",
    "test_predictions = [\n",
    "    make_prediction_example('Gauteng', 2024),\n",
    "    make_prediction_example('Eastern Cape', 2024),\n",
    "    make_prediction_example('Western Cape', 2025)\n",
    "]\n",
    "\n",
    "print('üß™ Example Predictions:')\n",
    "for pred in test_predictions:\n",
    "    if 'error' not in pred:\n",
    "        print(f\"   {pred['province']} ({pred['year']}): {pred['risk_score']:.2f} - {pred['risk_level']} Risk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions {#conclusions}\n",
    "\n",
    "### 9.1 Key Findings\n",
    "\n",
    "Our analysis reveals several important insights about service delivery protests in South Africa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print('üîç KEY FINDINGS SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "print('üìä MODEL PERFORMANCE:')\n",
    "print(f'   ‚Ä¢ Best Algorithm: {best_model_name}')\n",
    "print(f'   ‚Ä¢ R¬≤ Score: {results[best_model_name][\"r2\"]:.3f}')\n",
    "print(f'   ‚Ä¢ Cross-validation R¬≤: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}')\n",
    "print(f'   ‚Ä¢ RMSE: {results[best_model_name][\"rmse\"]:.3f}')\n",
    "\n",
    "print('üéØ TOP PREDICTIVE FACTORS:')\n",
    "for i, (_, row) in enumerate(importance_df.head(5).iterrows(), 1):\n",
    "    print(f'   {i}. {row[\"Feature\"]}: {row[\"Importance\"]:.3f}')\n",
    "\n",
    "print('üó∫Ô∏è PROVINCIAL RISK LEVELS:')\n",
    "high_risk = [p for p, r in provincial_predictions.items() if r > 6]\n",
    "medium_risk = [p for p, r in provincial_predictions.items() if 3 < r <= 6]\n",
    "low_risk = [p for p, r in provincial_predictions.items() if r <= 3]\n",
    "\n",
    "print(f'   ‚Ä¢ High Risk ({len(high_risk)}): {\", \".join(high_risk)}')\n",
    "print(f'   ‚Ä¢ Medium Risk ({len(medium_risk)}): {\", \".join(medium_risk)}')\n",
    "print(f'   ‚Ä¢ Low Risk ({len(low_risk)}): {\", \".join(low_risk)}')\n",
    "\n",
    "print('üí° POLICY RECOMMENDATIONS:')\n",
    "print('   ‚Ä¢ Focus on service delivery gaps as primary intervention point')\n",
    "print('   ‚Ä¢ Prioritize refuse collection services in high-risk areas')\n",
    "print('   ‚Ä¢ Implement early warning systems based on service delivery index')\n",
    "print('   ‚Ä¢ Target resource allocation to provinces with highest predicted risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Model Limitations and Future Work\n",
    "\n",
    "**Limitations:**\n",
    "- Limited historical protest data may affect model generalization\n",
    "- Provincial-level aggregation may mask local variations\n",
    "- Model assumes linear relationships between service delivery and protest risk\n",
    "\n",
    "**Future Improvements:**\n",
    "- Incorporate real-time social media sentiment analysis\n",
    "- Add economic indicators and unemployment data\n",
    "- Develop municipality-level predictions\n",
    "- Include seasonal and temporal patterns\n",
    "\n",
    "### 9.3 Business Impact\n",
    "\n",
    "This model provides government and policy makers with:\n",
    "- **Early Warning System**: Identify high-risk areas before protests occur\n",
    "- **Resource Allocation**: Prioritize service delivery improvements\n",
    "- **Policy Planning**: Data-driven decision making for service delivery\n",
    "- **Performance Monitoring**: Track service delivery effectiveness over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model summary\n",
    "print('='*60)\n",
    "print('         SERVICE DELIVERY PROTEST PREDICTION MODEL')\n",
    "print('                    COMPETITION SUBMISSION')\n",
    "print('='*60)\n",
    "\n",
    "print('‚úÖ DELIVERABLES COMPLETED:')\n",
    "print('   üìä Data Understanding & Exploration')\n",
    "print('   üßπ Data Cleaning & Preprocessing')\n",
    "print('   üìà Exploratory Data Analysis & Visualization')\n",
    "print('   üõ†Ô∏è Feature Engineering')\n",
    "print('   ü§ñ Model Selection, Training & Testing')\n",
    "print('   üìã Model Evaluation & Validation')\n",
    "print('   üìä Analysis, Results & Interpretation')\n",
    "print('   üöÄ Model Deployment (Streamlit Dashboard)')\n",
    "print('   üìù Code Documentation & Organization')\n",
    "print('   üìì Comprehensive Jupyter Notebook')\n",
    "\n",
    "print('üèÜ COMPETITION READY!')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
